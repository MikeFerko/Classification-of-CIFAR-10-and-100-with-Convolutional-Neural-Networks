{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"LAB 4-Classification with CNN.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Za6i8HdTM9jH","colab_type":"text"},"source":["<div style=\"text-align: center;font-weight:bold; font-size:300%; LINE-HEIGHT:125%\">\n","    <br>Deep Learning ECE 614<br>\n","</div>\n","<div style=\"text-align: center;font-weight:bold; font-size:200%; LINE-HEIGHT:125%\"> \n","        Laboratory #4<br>\n","        Spring 2020<br> \n","        Classification of MNIST Digits with Convolutional Neural Networks (CNN)<br>\n","</div>\n","<div style=\"text-align: center; font-size:100%\"> \n","        <br><b>Objective:</b> To understand and implement a classification of 2D images with convolutional neural networks.\n","</div>\n","\n","\n","## Tasks\n","1. Run the code as given. Do this cell by cell by clicking the Run button in the tool bar above. All cells can be run by a command in the \"Cell\" drop down menu. Note the number of parameters in this network compared to Lab #2.\n","2. In the visualization code segment, add an additional plot for the accuracy of the model vs training epoch.\n","3. Experiment with changing hyperparameters such as: convolutional kernel size, number of kernels, depth of network (number of layers), number of epochs. Compare results with others in this lab. Comment on accuracy of this LAB classification results as compared to earlier LABs.\n"]},{"cell_type":"markdown","metadata":{"id":"Lpj8Aar1M9jJ","colab_type":"text"},"source":["## Imports\n","This lab will use the numpy for general and matrix mathematical functions, matplotlib for visualization, and  Keras for AI algorithms."]},{"cell_type":"code","metadata":{"id":"G1xzvR67M9jL","colab_type":"code","outputId":"ad54f23c-34cc-4e84-fc74-9a4549502f3c","executionInfo":{"status":"ok","timestamp":1587347937853,"user_tz":240,"elapsed":2319,"user":{"displayName":"Michael Ferko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjozUEIFPZxC8r5v22Sgk255XmWl1i6Av33_7Cyg=s64","userId":"02782317569504334217"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","from tensorflow.python.keras.datasets import mnist\n","from tensorflow.python.keras.layers.core import Dense\n","from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout\n","from tensorflow.python.keras.layers import Input\n","from tensorflow.python.keras.models import Sequential\n","from keras.utils import to_categorical\n","from tensorflow.python.keras import optimizers\n","from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras import backend as K"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"xwJYoCfzM9jR","colab_type":"text"},"source":["## Data Loading and Reshaping\n","Keras provides the MNIST dataset from here: https://keras.io/datasets/, more info here http://yann.lecun.com/exdb/mnist/\n","\n","Keras models expect inputs to be tensors, represented as multidimensional arrays. The data should be formatted such that each sample is a row (index in the first dimension) of the array. The MNIST dataset is natively in the form of an M x W x H array, where M is the number of samples, W is the width of each image, and H is the height of each image. This is almost correct for the convolutional layers that will be used in this lab. However, they expect a fourth order tensor in the form of M x W x H x C, where C is the number of color channels in the image and the other dimensions are as before. For a standard RGB image this dimension would be size 3. The C dimension is still required even if the image is grayscale (one color channel). Information on the function used to perform this change can be found here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html#numpy.reshape\n","\n","The output data must also be vectorized. By default each class label is an integer between 0-9. For training, a target vector of length equal to the number of classes with a 1 in the correct class index is required. Keras has built in utilities to do this conversion. Observe the output below for the first nine samples in the MNIST dataset https://keras.io/utils/#to_categorical"]},{"cell_type":"code","metadata":{"id":"NLsIOs_8M9jT","colab_type":"code","outputId":"9885c9fd-ddb5-4131-8da0-cff8d86d38ef","executionInfo":{"status":"ok","timestamp":1587347939225,"user_tz":240,"elapsed":3626,"user":{"displayName":"Michael Ferko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjozUEIFPZxC8r5v22Sgk255XmWl1i6Av33_7Cyg=s64","userId":"02782317569504334217"}},"colab":{"base_uri":"https://localhost:8080/","height":473}},"source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","fig = plt.figure()\n","for i in range(9):\n","    plt.subplot(3,3,i+1)\n","    plt.tight_layout()\n","    plt.imshow(x_train[i], cmap='gray', interpolation='none')\n","    plt.title(\"Digit: {}\".format(y_train[i]))\n","    plt.xticks([])\n","    plt.yticks([])\n","plt.show()\n","    \n","x_train = x_train.reshape(60000, 28, 28, 1)\n","x_test = x_test.reshape(10000, 28, 28, 1)\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","x_train = x_train/255\n","x_test = x_test/255\n","\n","num_classes = 10\n","print('y data before: ')\n","print(y_train[0:5])\n","\n","y_train = to_categorical(y_train, num_classes)\n","y_test  = to_categorical(y_test, num_classes)\n","print('\\ny data after:')\n","print(y_train[0:5])\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["y data before: \n","[5 0 4 1 9]\n","\n","y data after:\n","[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XFP8yQldM9ja","colab_type":"text"},"source":["## Model Setup\n","![classification%20cnn.png]See Fig. Model, separate(attachment:classification%20cnn.png)\n","For this laboratory the loss, output layer, and metrics will be the same as in Laboratory 2, Classification with MLPs. The biggest change will be in the architecture of the model now using convolutional layers to extract image features.\n","\n","Previously when preparing the data for use with MLPs, the images were reshaped (dimensionality changed) into vectors. By doing this, explicit adjacency information or planar features or correlations were lost. The convolutional layers keep and utilize this information, learning context around each pixel.\n","\n","Keras has the capabililty for convolutional layers in 1D, 2D, or 3D. Since the input data are planar patterns (2D images), 2D convolutions will be used. 1D convolutions can be used for 1D signal analysis applications such as audio, text, etc. 3D convolutions are heavily used in medical image (MRI, CT, etc.) analysis. As a side note, most deep learning packages actually do correlation instead of convolution, but the difference is only transposing the kernel, so no practical difference in this application.\n","\n","Adding a convolutional layer is similar to a Dense layer, the first argument is the number of unique kernels for this layer. The second required argument is the size of the kernel. This is generally a tuple of odd numbered integers ((n, n) for 2D convolution). This lab starts with 5x5 and 3x3 kernels respectively. https://keras.io/layers/convolutional/#conv2d\n","\n","Another layer introduced here is the MaxPooling layer. This layer is used to downsample the image by just taking the maximum value in a neighborhood of pixels. The first argument to the layer is the size of this neighborhood. By default the operation is strided by the same value. Meaning a 2x2 neighborhood will result in a half sized output. https://keras.io/layers/pooling/#maxpooling2d\n","\n","The final new layer is the Flatten Layer. It reduces the dimensionality of the previous layers output to a vector. https://keras.io/layers/core/#flatten\n","The concept behind this is that the convolutional layers will extract abstract features from the images, such as edges, shapes, or complex features like eyes and faces. Then a MLP network is used to separate these high level features into their respective classes. \n","\n","As before in Lab #2, Cross Entropy (CE) will be used as loss, and the accuracy metric will be used to judge the percentage of correct predictions from the network.\n","$$CE = -\\sum_{i}^C t_i\\ln{s_i}  $$\n","\n","$$\\DeclareMathOperator*{\\argmax}{arg\\,max} Acc = \\frac{1}{M}\\sum_{k}^M\\argmax{t_k} == \\argmax{s_k} $$"]},{"cell_type":"code","metadata":{"id":"JOJ0koFXM9jb","colab_type":"code","outputId":"16a7ab47-20e7-4981-b05a-a7a94c295ea9","executionInfo":{"status":"ok","timestamp":1587347939685,"user_tz":240,"elapsed":4071,"user":{"displayName":"Michael Ferko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjozUEIFPZxC8r5v22Sgk255XmWl1i6Av33_7Cyg=s64","userId":"02782317569504334217"}},"colab":{"base_uri":"https://localhost:8080/","height":396}},"source":["K.clear_session()\n","model = Sequential()\n","model.add(Conv2D(8, (9,9), activation='relu', input_shape=(28,28,1,)))\n","model.add(MaxPooling2D((2,2), name='pool_1'))\n","model.add(Conv2D(16, (5,5), activation='relu'))\n","model.add(MaxPooling2D((2,2), name='pool_2'))\n","model.add(Flatten())\n","model.add(Dense(16, activation='relu'))\n","model.add(Dense(10, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","model.summary()\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 20, 20, 8)         656       \n","_________________________________________________________________\n","pool_1 (MaxPooling2D)        (None, 10, 10, 8)         0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 6, 6, 16)          3216      \n","_________________________________________________________________\n","pool_2 (MaxPooling2D)        (None, 3, 3, 16)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 144)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 16)                2320      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                170       \n","=================================================================\n","Total params: 6,362\n","Trainable params: 6,362\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OpCVgfxcM9jf","colab_type":"text"},"source":["## Model Training\n","The first two arguments of the fit function are the input and target data, followed by the number of epochs, and the batch size. The optional 'verbose' argument can be used to turn off the output text. Many of the other options are available and will be used in future laboratories. This example also uses validation data from the MNIST test set. The loss and metrics will be calculated for this data, however they are not used to directly adjust weights.\n","\n","\n","To start, training will be performed for 10 epochs with a batch size of 128, on 10,000 of the training images. 1,000 of the test images will be used for validation. This reduced dataset is to increase training speed. If time permits, try training/testing on the full dataset.\n","\n","Options for compiling, fitting, and evaluating the model can be found here.\n","https://keras.io/models/model/"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"2dP953YNM9jf","colab_type":"code","outputId":"6f7ede17-025e-44d3-98d5-621a42fa6970","executionInfo":{"status":"ok","timestamp":1586203210194,"user_tz":240,"elapsed":578742,"user":{"displayName":"Michael Ferko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjozUEIFPZxC8r5v22Sgk255XmWl1i6Av33_7Cyg=s64","userId":"02782317569504334217"}},"colab":{"base_uri":"https://localhost:8080/","height":396}},"source":["training_samples = 60000\n","testing_samples  = 10000\n","\n","batch_size = 128\n","epochs     = 25\n","\n","history = model.fit(x_train[:training_samples],\n","                    y_train[:training_samples],\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    verbose=1,\n","                    validation_data=(x_test[:testing_samples],y_test[:testing_samples]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","469/469 [==============================] - 24s 51ms/step - loss: 0.5223 - accuracy: 0.8356 - val_loss: 0.1486 - val_accuracy: 0.9551\n","Epoch 2/25\n","469/469 [==============================] - 24s 51ms/step - loss: 0.1357 - accuracy: 0.9596 - val_loss: 0.1101 - val_accuracy: 0.9631\n","Epoch 3/25\n","469/469 [==============================] - 23s 50ms/step - loss: 0.0985 - accuracy: 0.9701 - val_loss: 0.0762 - val_accuracy: 0.9761\n","Epoch 4/25\n","469/469 [==============================] - 23s 50ms/step - loss: 0.0813 - accuracy: 0.9756 - val_loss: 0.0696 - val_accuracy: 0.9795\n","Epoch 5/25\n","469/469 [==============================] - 24s 51ms/step - loss: 0.0723 - accuracy: 0.9778 - val_loss: 0.0606 - val_accuracy: 0.9819\n","Epoch 6/25\n","469/469 [==============================] - 24s 51ms/step - loss: 0.0639 - accuracy: 0.9803 - val_loss: 0.0547 - val_accuracy: 0.9831\n","Epoch 7/25\n","469/469 [==============================] - 24s 50ms/step - loss: 0.0583 - accuracy: 0.9816 - val_loss: 0.0491 - val_accuracy: 0.9850\n","Epoch 8/25\n","469/469 [==============================] - 23s 50ms/step - loss: 0.0531 - accuracy: 0.9839 - val_loss: 0.0441 - val_accuracy: 0.9859\n","Epoch 9/25\n","469/469 [==============================] - 23s 49ms/step - loss: 0.0493 - accuracy: 0.9847 - val_loss: 0.0483 - val_accuracy: 0.9845\n","Epoch 10/25\n","469/469 [==============================] - 23s 50ms/step - loss: 0.0454 - accuracy: 0.9862 - val_loss: 0.0486 - val_accuracy: 0.9844\n","Epoch 11/25\n","357/469 [=====================>........] - ETA: 5s - loss: 0.0427 - accuracy: 0.9869"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XgpOjO9uM9jk","colab_type":"text"},"source":["## Visualization\n","The %matplotlib qt will make a new, interactive window. Change the qt to inline for figures to appear in this window, but they will not be interactive.\n","\n","The below code segment outputs two plots from the training performed in the previous code segment. These show the average loss across the training and validation datasets for each epoch. Using the same 'history' object add an additional plot for the average training and validation accuracy metric across each epoch."]},{"cell_type":"code","metadata":{"id":"z854gdORM9jk","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","\n","plt.figure()\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","\n","plt.title('Model loss')\n","plt.ylabel('loss'); plt.xlabel('epoch')\n","plt.legend(('train','validation'))\n","\n","#### Students to add accuracy plot here ####\n","\n","fig1 = plt.figure()\n","ax1 = fig1.add_subplot(111)\n","ax1.plot(history.history['accuracy'])\n","ax1.plot(history.history['val_accuracy'])\n","ax1.title.set_text('Model Accuracy')\n","plt.ylabel('Accuracy'); plt.xlabel('epoch')\n","ax1.legend(('train','validation'))\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_achFquM9jo","colab_type":"text"},"source":["## Feature Maps\n","\n","Using the Functional model building in Keras, new models are built using intermediate layers from the trained model. A sample set of these outputs is generated by the code below from these layers for visualization of features."]},{"cell_type":"code","metadata":{"id":"_yOOiZACM9jq","colab_type":"code","colab":{}},"source":["low_level_layer = Model(inputs=model.input,\n","                        outputs=model.layers[0].output)\n","print(low_level_layer)\n","high_level_layer = Model(inputs=model.input,\n","                         outputs = model.layers[2].output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"44aLvR1GM9jw","colab_type":"code","colab":{}},"source":["sample_set = 6\n","\n","low_level_features = low_level_layer.predict(x_train[:sample_set])\n","high_level_features = high_level_layer.predict(x_train[:sample_set])\n","\n","for sample in range(sample_set):\n","    plt.figure()\n","    plt.imshow(x_train[sample][:,:,0])\n","    fig = plt.figure(figsize=[12.8,3])\n","    for i in range(8):\n","        \n","        feature_map = low_level_features[sample, :, :, i]\n","\n","        plt.subplot(2, 8, i+1)\n","        plt.imshow(feature_map, interpolation='none')\n","        plt.axis('off')\n","    for i in range(8):\n","\n","        feature_map = high_level_features[sample, :, :, i]\n","\n","        plt.subplot(2, 8, i+9)\n","        plt.imshow(feature_map, interpolation='none')\n","        plt.axis('off')\n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snsCfogjM9j1","colab_type":"text"},"source":["## Appendix A - Log Loss"]},{"cell_type":"code","metadata":{"id":"dKvq5i5HM9j2","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x = np.linspace(1e-6,2, 100)\n","\n","fig = plt.figure(figsize=[10,5])\n","plt.plot(x,-np.log(x))\n","plt.grid(True)\n","plt.title('Log loss (Cross Entropy)')\n","plt.xlabel('x')\n","plt.ylabel('-ln(x)')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQALhIguM9j7","colab_type":"text"},"source":["## Appendix B - Activation Functions"]},{"cell_type":"code","metadata":{"id":"cvgWN2sOM9j8","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.activations import relu, tanh, sigmoid, hard_sigmoid \n","from keras.backend import eval\n","\n","x = np.linspace(-5,5,100)\n","\n","fig = plt.figure(figsize=[20,4.8])\n","plt.subplot(1,3,1)\n","plt.plot(x, eval(sigmoid(x)))\n","plt.ylabel('Sigmoid')\n","plt.grid(True)\n","\n","plt.subplot(1,3,2)\n","plt.plot(x, eval(tanh(x)))\n","plt.ylabel('tanh')\n","plt.grid(True)\n","plt.title('Example Activation Functions')\n","\n","plt.subplot(1,3,3)\n","plt.plot(x, eval(relu(x)))\n","plt.ylabel('ReLU')\n","plt.grid(True)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h9NG4yysM9kB","colab_type":"text"},"source":["$$\\textrm{Sigmoid}(x) = \\frac{1}{1+e^{-x}}$$\n","\n","\n","$$\\textrm{tanh}(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$$\n","\n","\n","$$\\textrm{ReLU}(x) = \\max(0,x)$$"]},{"cell_type":"code","metadata":{"id":"8YscmiPXM9kC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}